{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9deb867c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Setup and Imports\n",
    "# ==========================================\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Import data utilities\n",
    "from src.data_utils import load_dataset\n",
    "\n",
    "print(\"Imports complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9be09e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 102,450 names from names_group3.txt\n",
      "Vocabulary size: 27 characters\n",
      "   Characters: abcdefghijklmnopqrstuvwxyz\n",
      "Dataset splits:\n",
      "   Train: 81,960 names (80%)\n",
      "   Val:   10,245 names (10%)\n",
      "   Test:  10,245 names (10%)\n",
      "\n",
      "==================================================\n",
      "DATASET STATISTICS\n",
      "==================================================\n",
      "Total names:    102,450\n",
      "Train names:    81,960\n",
      "Val names:      10,245\n",
      "Test names:     10,245\n",
      "Vocabulary:     27 characters\n",
      "Name length:    min=2, max=15, avg=6.5\n",
      "==================================================\n",
      "\n",
      "Dataset loaded: 81,960 training names\n",
      "Vocabulary size: 27\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Load Dataset\n",
    "# ==========================================\n",
    "\n",
    "dataset = load_dataset('../data/processed/names_group3.txt')\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset.train_names):,} training names\")\n",
    "print(f\"Vocabulary size: {dataset.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15a8be21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation function ready!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Name Generation Function\n",
    "# ==========================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_name(max_length=20):\n",
    "    \"\"\"Generate a name using the trained WaveNet model.\"\"\"\n",
    "    name = []\n",
    "    context = [dataset.stoi['.']] * block_size\n",
    "    \n",
    "    while True:\n",
    "        # Prepare input\n",
    "        x = torch.tensor([context])\n",
    "        \n",
    "        # Forward pass\n",
    "        emb = C[x]\n",
    "        embcat = emb.view(1, -1)\n",
    "        \n",
    "        # Layer 1\n",
    "        hpreact1 = embcat @ W1 + b1\n",
    "        hpreact1 = bngain1 * (hpreact1 - bnmean_running1) / bnstd_running1 + bnbias1\n",
    "        h1 = torch.tanh(hpreact1)\n",
    "        \n",
    "        # Layer 2\n",
    "        hpreact2 = h1 @ W2 + b2\n",
    "        hpreact2 = bngain2 * (hpreact2 - bnmean_running2) / bnstd_running2 + bnbias2\n",
    "        h2 = torch.tanh(hpreact2)\n",
    "        \n",
    "        # Layer 3\n",
    "        hpreact3 = h2 @ W3 + b3\n",
    "        hpreact3 = bngain3 * (hpreact3 - bnmean_running3) / bnstd_running3 + bnbias3\n",
    "        h3 = torch.tanh(hpreact3)\n",
    "        \n",
    "        # Output\n",
    "        logits = h3 @ Wout + bout\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        # Sample next character\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        \n",
    "        # Stop at period\n",
    "        if ix == dataset.stoi['.']:\n",
    "            break\n",
    "        \n",
    "        # Add to name\n",
    "        name.append(dataset.itos[ix])\n",
    "        \n",
    "        # Update context\n",
    "        context = context[1:] + [ix]\n",
    "        \n",
    "        if len(name) >= max_length:\n",
    "            break\n",
    "    \n",
    "    return ''.join(name)\n",
    "\n",
    "print(\"Generation function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f082b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Load Best Model (WaveNet block_size=10)\n",
    "# ==========================================\n",
    "\n",
    "# Hyperparameters (must match training)\n",
    "block_size = 10\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "print(\"Loading WaveNet model (block_size=10)...\")\n",
    "\n",
    "# Load model checkpoint\n",
    "checkpoint = torch.load('../outputs/wavenet_block10_model.pth')\n",
    "\n",
    "# Extract parameters\n",
    "C = checkpoint['C']\n",
    "W1 = checkpoint['W1']\n",
    "b1 = checkpoint['b1']\n",
    "bngain1 = checkpoint['bngain1']\n",
    "bnbias1 = checkpoint['bnbias1']\n",
    "bnmean_running1 = checkpoint['bnmean_running1']\n",
    "bnstd_running1 = checkpoint['bnstd_running1']\n",
    "\n",
    "W2 = checkpoint['W2']\n",
    "b2 = checkpoint['b2']\n",
    "bngain2 = checkpoint['bngain2']\n",
    "bnbias2 = checkpoint['bnbias2']\n",
    "bnmean_running2 = checkpoint['bnmean_running2']\n",
    "bnstd_running2 = checkpoint['bnstd_running2']\n",
    "\n",
    "W3 = checkpoint['W3']\n",
    "b3 = checkpoint['b3']\n",
    "bngain3 = checkpoint['bngain3']\n",
    "bnbias3 = checkpoint['bnbias3']\n",
    "bnmean_running3 = checkpoint['bnmean_running3']\n",
    "bnstd_running3 = checkpoint['bnstd_running3']\n",
    "\n",
    "Wout = checkpoint['Wout']\n",
    "bout = checkpoint['bout']\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"   Val Loss: {checkpoint.get('val_loss', 'N/A')}\")\n",
    "print(f\"   Parameters: {checkpoint.get('total_params', 'N/A'):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8524d7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REAL-TIME NAME GENERATION DEMO\n",
      "================================================================================\n",
      "\n",
      "Watch the model generate names character-by-character!\n",
      "\n",
      "ðŸŽ² Generating 10 names in real-time...\n",
      "\n",
      " 1.    Generating: finnadeyas \n",
      " 2.    Generating: teiden \n",
      " 3.    Generating: desie \n",
      " 4.    Generating: youn \n",
      " 5.    Generating: ramon \n",
      " 6.    Generating: gurd \n",
      " 7.    Generating: guci \n",
      " 8.    Generating: lexe \n",
      " 9.    Generating: melia \n",
      "10.    Generating: aylen \n",
      "\n",
      "================================================================================\n",
      "Real-time generation complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# REAL-TIME NAME GENERATION\n",
    "# ==========================================\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"REAL-TIME NAME GENERATION DEMO\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nWatch the model generate names character-by-character!\\n\")\n",
    "\n",
    "def generate_name_with_display(max_length=20):\n",
    "    \"\"\"Generate name and display character-by-character.\"\"\"\n",
    "    name = []\n",
    "    context = [dataset.stoi['.']] * block_size\n",
    "    \n",
    "    print(\"   Generating: \", end=\"\", flush=True)\n",
    "    \n",
    "    while True:\n",
    "        x = torch.tensor([context])\n",
    "        \n",
    "        # Forward pass\n",
    "        emb = C[x]\n",
    "        embcat = emb.view(1, -1)\n",
    "        \n",
    "        hpreact1 = embcat @ W1 + b1\n",
    "        hpreact1 = bngain1 * (hpreact1 - bnmean_running1) / bnstd_running1 + bnbias1\n",
    "        h1 = torch.tanh(hpreact1)\n",
    "        \n",
    "        hpreact2 = h1 @ W2 + b2\n",
    "        hpreact2 = bngain2 * (hpreact2 - bnmean_running2) / bnstd_running2 + bnbias2\n",
    "        h2 = torch.tanh(hpreact2)\n",
    "        \n",
    "        hpreact3 = h2 @ W3 + b3\n",
    "        hpreact3 = bngain3 * (hpreact3 - bnmean_running3) / bnstd_running3 + bnbias3\n",
    "        h3 = torch.tanh(hpreact3)\n",
    "        \n",
    "        logits = h3 @ Wout + bout\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        \n",
    "        if ix == dataset.stoi['.']:\n",
    "            break\n",
    "        \n",
    "        char = dataset.itos[ix]\n",
    "        name.append(char)\n",
    "        print(char, end=\"\", flush=True)\n",
    "        time.sleep(0.1)  # Dramatic pause\n",
    "        \n",
    "        context = context[1:] + [ix]\n",
    "        \n",
    "        if len(name) >= max_length:\n",
    "            break\n",
    "    \n",
    "    print(\" \")\n",
    "    return ''.join(name)\n",
    "\n",
    "# Generate 10 names with animation\n",
    "print(\"Generating 10 names in real-time...\\n\")\n",
    "demo_names = []\n",
    "for i in range(10):\n",
    "    print(f\"{i+1:2d}.\", end=\" \")\n",
    "    name = generate_name_with_display()\n",
    "    demo_names.append(name)\n",
    "    time.sleep(0.3)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Real-time generation complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6689a165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ² Generating 30 additional names quickly...\n",
      "\n",
      " 1. lataria              2. kenisa               3. nessicit            \n",
      " 4. cathcia              5. coorin               6. brishaet            \n",
      " 7. risha                8. caveettt             9. sevir               \n",
      "10. brit                11. izmon               12. viombel             \n",
      "13. jaissa              14. miann               15. naraabeea           \n",
      "16. tsofana             17. berneven            18. amyaen              \n",
      "19. abjan               20. couren              21. bashe               \n",
      "22. indye               23. gurika              24. clemanis            \n",
      "25. bawleng             26. devei               27. brittaymi           \n",
      "28. arlangede           29. arighett            30. johneve             \n",
      "\n",
      "================================================================================\n",
      "âœ… Generated 30 more names!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# BATCH GENERATION (30 Names)\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\nGenerating 30 additional names quickly...\\n\")\n",
    "\n",
    "more_names = []\n",
    "for i in range(30):\n",
    "    name = generate_name()\n",
    "    more_names.append(name)\n",
    "    print(f\"{i+1:2d}. {name:<20}\", end=\"\")\n",
    "    if (i + 1) % 3 == 0:\n",
    "        print()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Generated {len(more_names)} more names!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d02aace2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GENERATION STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Total names generated: 10\n",
      "Average length: 5.2 characters\n",
      "Shortest: 4 chars ('youn')\n",
      "Longest: 10 chars ('finnadeyas')\n",
      "Unique names: 10/10\n",
      "\n",
      "Length Distribution:\n",
      "   4 chars: â–ˆâ–ˆâ–ˆâ–ˆ (4)\n",
      "   5 chars: â–ˆâ–ˆâ–ˆâ–ˆ (4)\n",
      "   6 chars: â–ˆ (1)\n",
      "  10 chars: â–ˆ (1)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# GENERATION STATISTICS\n",
    "# ==========================================\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Combine all generated names\n",
    "\n",
    "print(\"\\nGENERATION STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "name_lengths = [len(name) for name in demo_names]\n",
    "\n",
    "print(f\"\\nTotal names generated: {len(demo_names)}\")\n",
    "print(f\"Average length: {np.mean(name_lengths):.1f} characters\")\n",
    "print(f\"Shortest: {min(name_lengths)} chars ('{min(demo_names, key=len)}')\")\n",
    "print(f\"Longest: {max(name_lengths)} chars ('{max(demo_names, key=len)}')\")\n",
    "print(f\"Unique names: {len(set(demo_names))}/{len(demo_names)}\")\n",
    "\n",
    "# Length distribution\n",
    "print(f\"\\nLength Distribution:\")\n",
    "length_counts = Counter(name_lengths)\n",
    "for length in sorted(length_counts.keys()):\n",
    "    bar = \"â–ˆ\" * length_counts[length]\n",
    "    print(f\"  {length:2d} chars: {bar} ({length_counts[length]})\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ecad93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 10 names to: ../outputs/demo_generated_names.txt\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# SAVE DEMO NAMES\n",
    "# ==========================================\n",
    "\n",
    "# Get values from checkpoint (loaded earlier)\n",
    "val_loss = checkpoint['val_loss']\n",
    "total_params = checkpoint['total_params']\n",
    "\n",
    "# Save to file\n",
    "output_file = '../outputs/demo_generated_names.txt'\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write(\"makemore - Generated Names Demo\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    f.write(f\"Model: WaveNet (block_size={block_size})\\n\")\n",
    "    f.write(f\"Val Loss: {val_loss:.4f}\\n\")\n",
    "    f.write(f\"Parameters: {total_params:,}\\n\\n\")\n",
    "    f.write(\"Generated Names:\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    for i, name in enumerate(demo_names, 1):\n",
    "        f.write(f\"{i:3d}. {name}\\n\")\n",
    "\n",
    "print(f\"\\nSaved {len(demo_names)} names to: {output_file}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077f51fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb38bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
